# -*- coding: utf-8 -*-
"""Project_Whisper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1akxKPVKyV01E2SYnca9ZXTw21oqfD6oK
"""

!pip install faster-whisper

!pip install yt-dlp

!pip install hf_xet

!pip install imageio[ffmpeg]

!pip install -U youtube-transcript-api

pip uninstall youtube-transcript-api -y

pip install youtube-transcript-api==0.6.1

pip install git+https://github.com/openai/whisper.git

!pip install imageio-ffmpeg

import sys
print(sys.executable)

import sys
!{sys.executable} -m pip install --upgrade pip
!{sys.executable} -m pip install git+https://github.com/openai/whisper.git
!{sys.executable} -m pip install yt-dlp ffmpeg-python imageio-ffmpeg

!pip install youtube-transcript-api

import os
os.environ["HTTPS_PROXY"] = ""
os.environ["HTTP_PROXY"] = ""

print("Hello, Jupyter!")

!pip install unidecode

!pip install yt_dlp

import yt_dlp
print("yt_dlp installed successfully!")

pip install yt-dlp tqdm unidecode requests youtube-transcript-api faster-whisper

from concurrent.futures import ThreadPoolExecutor
print("OK!")

# ===== Notebook Configuration =====
# Replace with your own keys/files as needed.
API_KEY = "AIzaSyATQmwvbBLtYW4zfC8XMJMtRrN-LAvU_Ek"  # or your own YT API key
CHANNEL_ID = "UC4qz5w2M-Xmju7WC9ynqRtw"
MAX_RESULTS = 700

# Path configuration (use an absolute path if mounting Drive)
OUT_DIR = "/content/t"
CACHE_DIR = "/content/cache"
SUMMARY_JSON = "/content/transcript_summary.json"
FAIL_CSV = "/content/transcript_failures.csv"
PROCESSED_JSON = "/content/processed.json"
LOG_FILE = "/content/transcript.log"

# Tunables
MAX_WORKERS = 4       # lower this on small runtimes
BASE_SLEEP = 1.0
RETRY_COUNT = 3
TRANSCRIPT_RETRIES = 4
TRANSCRIPT_BACKOFF_BASE = 1.0
TRANSCRIPT_THROTTLE = 0.25

# Whisper device: set to "cuda" if you enabled GPU runtime in Colab
DEVICE = "cuda" if ( "gpu" in __import__("os").environ.get("COLAB_GPU", "") or __import__("torch").cuda.is_available() ) else "cpu"
# If torch isn't available, default to 'cpu'
try:
    import torch
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
except Exception:
    DEVICE = "cpu"

# Hugging Face token (optional) - set to None or your token string
HF_TOKEN = None  # e.g., "hf_XXXXXXXXXXXXXXXXXXXXXXXX"

# Cookies file path (optional) - if you have cookies.txt uploaded to Colab
COOKIES_FILE = None  # e.g., "/content/cookies.txt"

# Create directories
import os
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

print("CONFIG")
print("  OUT_DIR:", OUT_DIR)
print("  CACHE_DIR:", CACHE_DIR)
print("  DEVICE:", DEVICE)

# ==== CONFIG (set before running the main script) ====
API_KEY = "AIzaSyATQmwvbBLtYW4zfC8XMJMtRrN-LAvU_Ek"
CHANNEL_ID = "UC4qz5w2M-Xmju7WC9ynqRtw"  # Example: Sri Sri Ravi Shankar
MAX_RESULTS = 200
  # how many recent videos to fetch
MAX_WORKERS = 4    # number of threads (4‚Äì8 works well for Colab)
WHISPER_DEVICE = "cpu"  # or "cuda" if GPU runtime
HF_TOKEN = None  # or your Hugging Face token for faster_whisper
COOKIES_FILE = None  # if you have cookies.txt (optional)

import os, requests

API_KEY = "AIzaSyC5c9BinbI0f635olewtOt3uLayNjvcWu4"  # üîπ Replace with your key
CHANNEL_ID = "UC4qz5w2M-Xmju7WC9ynqRtw"

url = "https://www.googleapis.com/youtube/v3/channels"
params = {"part": "contentDetails", "id": CHANNEL_ID, "key": API_KEY}
resp = requests.get(url, params=params)

print("Status:", resp.status_code)
print(resp.text[:500])

API_KEY = "AIzaSyC5c9BinbI0f635olewtOt3uLayNjvcWu4"

print("API_KEY:", API_KEY)

!pip install browser_cookie3

# ===== Enhanced YouTube Transcript Batch Processor for Colab =====
import os, re, csv, json, time, shutil, tempfile, logging, socket, threading, sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import requests
from unidecode import unidecode
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript
from faster_whisper import WhisperModel
import yt_dlp
import xml.parsers.expat

# ======== CONFIG / FALLBACKS ========
_globals = globals()
API_KEY = _globals.get("API_KEY", os.environ.get("YT_API_KEY", None))
CHANNEL_ID = _globals.get("CHANNEL_ID", os.environ.get("CHANNEL_ID", None))
MAX_RESULTS = int(_globals.get("MAX_RESULTS", os.environ.get("MAX_RESULTS", "700")))
BATCH_SIZE = int(_globals.get("BATCH_SIZE", 100))  # ‚ö° each batch size
DEFAULT_MAX_WORKERS = int(_globals.get("MAX_WORKERS", 4))

OUT_DIR = Path(_globals.get("OUT_DIR", "/content/t"))
CACHE_DIR = Path(_globals.get("CACHE_DIR", "/content/cache"))
SUMMARY_JSON = Path(_globals.get("SUMMARY_JSON", "/content/transcript_summary.json"))
FAIL_CSV = Path(_globals.get("FAIL_CSV", "/content/transcript_failures.csv"))
PROCESSED_JSON = Path(_globals.get("PROCESSED_JSON", "/content/processed.json"))
LOG_FILE = Path(_globals.get("LOG_FILE", "/content/transcript.log"))

BASE_SLEEP = 1.0
RETRY_COUNT = 3
TRANSCRIPT_RETRIES = 4
TRANSCRIPT_BACKOFF_BASE = 1.0
TRANSCRIPT_THROTTLE = 0.25
WHISPER_DEVICE = _globals.get("DEVICE", "cpu")
HF_TOKEN = _globals.get("HF_TOKEN", None)
COOKIES_FILE = _globals.get("COOKIES_FILE", None)

IN_COLAB = "google.colab" in sys.modules
OUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.FileHandler(str(LOG_FILE), encoding="utf-8"), logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("transcript")

processed_lock = threading.Lock()
transcript_api_lock = threading.Lock()
_last_transcript_call = 0.0

# ======= HELPERS =======
def safe_name(title):
    base = unidecode(title)
    base = re.sub(r"[^A-Za-z0-9_\- ]+", "", base).strip().replace(" ", "_")
    return base[:80] or "untitled"

def clamp_path(path: Path, max_total=230):
    if len(str(path)) <= max_total:
        return path
    stem, ext = path.stem, path.suffix
    while len(str(path)) > max_total and len(stem) > 10:
        stem = stem[:-5]
        path = path.with_name(stem + ext)
    return path

def extract_vid_from_url(url):
    if not url:
        return None
    m = re.search(r"(?:v=|youtu\.be/)([A-Za-z0-9_\-]{6,})", url)
    return m.group(1) if m else None

# ======= FETCH CHANNEL VIDEOS =======
def fetch_channel_videos(api_key, channel_id, max_results=1000):
    videos, next_page_token, retries = [], None, 0
    logger.info("[üîç Fetching uploads playlist ID]")
    channel_url = "https://www.googleapis.com/youtube/v3/channels"
    params = {"part": "contentDetails", "id": channel_id, "key": api_key}
    response = requests.get(channel_url, params=params)
    response.raise_for_status()
    uploads_id = response.json()["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
    logger.info(f"[üì∫ Uploads playlist ID]: {uploads_id}")

    while len(videos) < max_results:
        playlist_url = "https://www.googleapis.com/youtube/v3/playlistItems"
        params = {"part": "snippet", "playlistId": uploads_id, "key": api_key, "maxResults": 50}
        if next_page_token:
            params["pageToken"] = next_page_token
        try:
            data = requests.get(playlist_url, params=params, timeout=10).json()
        except Exception as e:
            retries += 1
            if retries <= RETRY_COUNT:
                time.sleep(BASE_SLEEP * retries)
                continue
            else:
                break
        for item in data.get("items", []):
            title = item["snippet"]["title"]
            vid = item["snippet"]["resourceId"]["videoId"]
            videos.append((title, f"https://www.youtube.com/watch?v={vid}"))
        next_page_token = data.get("nextPageToken")
        if not next_page_token:
            break
    logger.info(f"[‚úÖ Fetched {len(videos)} videos]")
    return videos[:max_results]

# ======= MODEL INIT =======
def init_whisper_model(hf_token, device):
    if hf_token:
        os.environ["HF_HUB_TOKEN"] = hf_token
    logger.info("Initializing Whisper model...")
    model = WhisperModel("tiny", device=device)
    logger.info("Whisper model ready.")
    return model

# ======= AUDIO DOWNLOAD + TRANSCRIPT =======
def _throttle_transcript_api():
    global _last_transcript_call
    with transcript_api_lock:
        now = time.time()
        elapsed = now - _last_transcript_call
        if elapsed < TRANSCRIPT_THROTTLE:
            time.sleep(TRANSCRIPT_THROTTLE - elapsed)
        _last_transcript_call = time.time()

def download_audio(video_id, url, path, cookies_file):
    opts = {
        "format": "bestaudio/best",
        "outtmpl": str(path),
        "quiet": True,
        "no_warnings": True,
        "ignoreerrors": True,
        "noplaylist": True,
    }
    if cookies_file:
        opts["cookiefile"] = cookies_file
    try:
        with yt_dlp.YoutubeDL(opts) as ydl:
            ydl.download([url])
        return path.exists() and path.stat().st_size > 0
    except Exception:
        return False

def fetch_transcript(video_id, url, model, cookies_file):
    for attempt in range(1, TRANSCRIPT_RETRIES + 1):
        try:
            _throttle_transcript_api()
            data = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
            if data:
                return "\n".join([s["text"].strip() for s in data if s["text"].strip()])
        except (TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript):
            break
        except Exception:
            pass
        if attempt < TRANSCRIPT_RETRIES:
            time.sleep(TRANSCRIPT_BACKOFF_BASE * (2 ** (attempt - 1)))
    # fallback Whisper
    audio_file = CACHE_DIR / f"{video_id}.mp3"
    if not audio_file.exists():
        ok = download_audio(video_id, url, audio_file, cookies_file)
        if not ok:
            return None
    try:
        segments, _ = model.transcribe(str(audio_file))
        return " ".join([seg.text for seg in segments]).strip()
    except Exception:
        return None

# ======= PROCESS TRACKING =======
def load_processed():
    if PROCESSED_JSON.exists():
        try:
            return json.loads(PROCESSED_JSON.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_processed(data):
    tmp = PROCESSED_JSON.with_suffix(".tmp")
    tmp.write_text(json.dumps(data, indent=2), encoding="utf-8")
    os.replace(tmp, PROCESSED_JSON)

# ======= PROCESS VIDEO =======
def process_video(item, processed, model, cookies_file):
    title, url = item
    vid = extract_vid_from_url(url)
    if not vid:
        return {"status": "invalid_url", "title": title}
    with processed_lock:
        if vid in processed:
            return {"status": "cached", "title": title}
    text = fetch_transcript(vid, url, model, cookies_file)
    if text:
        fname = clamp_path(OUT_DIR / f"{safe_name(title)}_{vid}.txt")
        with open(fname, "w", encoding="utf-8") as f:
            f.write(f"{title}\n{url}\n\n{text}")
        with processed_lock:
            processed[vid] = fname.name
            save_processed(processed)
        return {"status": "ok", "title": title}
    return {"status": "failed", "title": title, "vid": vid}

# ======= MAIN RUNNER WITH BATCHES =======
def run_main_batched(batch_size=BATCH_SIZE, max_workers=DEFAULT_MAX_WORKERS):
    model = init_whisper_model(HF_TOKEN, WHISPER_DEVICE)
    all_videos = fetch_channel_videos(API_KEY, CHANNEL_ID, MAX_RESULTS)
    processed = load_processed()
    logger.info(f"Already processed: {len(processed)}")

    unprocessed = [(t, u) for t, u in all_videos if extract_vid_from_url(u) not in processed]
    logger.info(f"Unprocessed videos: {len(unprocessed)}")

    total_batches = (len(unprocessed) + batch_size - 1) // batch_size
    logger.info(f"üîπ Splitting into {total_batches} batches of {batch_size} each")

    results = []
    for i in range(total_batches):
        batch = unprocessed[i*batch_size:(i+1)*batch_size]
        logger.info(f"\n=== üöÄ Starting batch {i+1}/{total_batches} ({len(batch)} videos) ===")
        batch_results = []
        with ThreadPoolExecutor(max_workers=max_workers) as exe:
            futures = [exe.submit(process_video, v, processed, model, COOKIES_FILE) for v in batch]
            for f in tqdm(as_completed(futures), total=len(batch), desc=f"Batch {i+1}/{total_batches}"):
                try:
                    batch_results.append(f.result())
                except Exception as e:
                    logger.exception(f"Error in batch {i+1}: {e}")
        results.extend(batch_results)
        save_processed(processed)
        logger.info(f"‚úÖ Batch {i+1} done ({len(batch_results)} videos). Autosaved.")
        time.sleep(2)  # prevent overload

    summary = {
        "total_videos": len(all_videos),
        "processed_total": len(processed),
        "batches": total_batches,
        "output_dir": str(OUT_DIR),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    SUMMARY_JSON.write_text(json.dumps(summary, indent=2))
    logger.info(json.dumps(summary, indent=2))
    if IN_COLAB:
        from google.colab import files
        shutil.make_archive("transcripts", "zip", OUT_DIR)
        files.download("transcripts.zip")
    return summary

# ======= RUN =======
print("Starting resumable batch transcription...")
_summary = run_main_batched()
print("‚úÖ All batches complete.")
print(json.dumps(_summary, indent=2))

response = requests.get(url, params=params)
data = response.json()
print(json.dumps(data, indent=2))  # üëà Add this to inspect the actual response

uploads_id = data["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]

from google.colab import files

files.download("/content/t")

import shutil
shutil.make_archive("transcripts", "zip", "t")

# --- Remove duplicate video entries by video ID ---
unique_videos = {}
for title, url in video_list:
    vid_match = re.search(r"(?:v=|youtu\.be/)([A-Za-z0-9_\-]{6,})", url)
    if vid_match:
        vid = vid_match.group(1)
        unique_videos[vid] = (title, url)  # Keeps the last one, discards earlier duplicates

if len(video_list) != len(unique_videos):
    print(f"‚ö†Ô∏è Removed {len(video_list) - len(unique_videos)} duplicate entries.")

video_list = list(unique_videos.values())